{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TxMM Code",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JellePiepenbrock/textmining2018/blob/master/TxMM_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "U6_2h652kSZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## This is a streamlined version of the code used to train models for the Text Mining Project: Cross-Genre Text Author Gender Prediction Using Logistic Regression and Bidirectional LSTM"
      ]
    },
    {
      "metadata": {
        "id": "q6CMBOzskeY0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### General imports"
      ]
    },
    {
      "metadata": {
        "id": "7CGoNGRSj5Uw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import re\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
        "import nltk\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.svm import SVC\n",
        "import sklearn\n",
        "import spacy\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# stop_words = set(stopwords.words('dutch'))\n",
        "!spacy download nl_core_news_sm\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nl942aHWkDow",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read data and Create test sets"
      ]
    },
    {
      "metadata": {
        "id": "w_s1FOP2kBbp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "newsloc = \"drive/My Drive/TextMining_Project/clin29/Training/GxG_News.txt\"\n",
        "twitterloc = \"drive/My Drive/TextMining_Project/clin29/Training/GxG_Twitter.txt\"\n",
        "youtubeloc = \"drive/My Drive/TextMining_Project/clin29/Training/GxG_YouTube.txt\"\n",
        "\n",
        "def parse_file(filelocation):\n",
        "\n",
        "  entries = []\n",
        "  identifiers = []\n",
        "  genders = []\n",
        "\n",
        "  for line in open(filelocation):\n",
        "    if line.startswith(\"<\"):\n",
        "      if line.startswith(\"<doc id\"):\n",
        "\n",
        "        identifier = line[9:12]\n",
        "        identifier = identifier.replace(\"\\\"\", \"\")\n",
        "        identifiers.append(int(identifier))\n",
        "\n",
        "        gender = re.search(r'gender', line)\n",
        "        gender = line[gender.end()+2:gender.end()+3]\n",
        "        genders.append(gender)\n",
        "\n",
        "        entry = []\n",
        "\n",
        "      if line.startswith(\"</doc>\"):\n",
        "        entries.append(''.join(entry))\n",
        "\n",
        "    else:\n",
        "      entry.append(line)\n",
        "\n",
        "  return entries, genders, identifiers\n",
        "\n",
        "  \n",
        "X_news, y_news, _ = parse_file(newsloc)\n",
        "X_tw, y_tw, _ = parse_file(twitterloc)\n",
        "X_yt, y_yt, _ = parse_file(youtubeloc)\n",
        "\n",
        "print(len(X_news), len(y_news))\n",
        "print(len(X_tw), len(y_tw))\n",
        "print(len(X_yt), len(y_yt))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-jeyjkenkPwF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for texts, genders, outputfolder in [(X_tw, y_tw, \"tw\"), (X_news, y_news, \"news\"), (X_yt, y_yt, \"yt\")]:\n",
        "  \n",
        "  length = len(texts)\n",
        "  print(length)\n",
        "  y_bin = np.array([0  if k==\"M\" else 1 for k in genders]).reshape((len(genders)))\n",
        "  print(len(y_bin))\n",
        "  p = np.random.permutation(len(y_bin))\n",
        "\n",
        "  x = pd.DataFrame(texts).iloc[p].values\n",
        "#   print(x)\n",
        "  y = y_bin[p]\n",
        "\n",
        "  training_x = x[:int(np.floor(0.8*length))]\n",
        "  training_y = y[:int(np.floor(0.8*length))]\n",
        "\n",
        "  val_x = x[int(np.floor(0.8*length)):int(np.floor(0.9*length))]\n",
        "  val_y = y[int(np.floor(0.8*length)):int(np.floor(0.9*length))]\n",
        "\n",
        "  test_x = x[int(np.floor(0.9*length)):]\n",
        "  test_y = y[int(np.floor(0.9*length)):]\n",
        "\n",
        "  training = pd.DataFrame()\n",
        "  val = pd.DataFrame()\n",
        "  test = pd.DataFrame()\n",
        "\n",
        "  training['label'] = training_y\n",
        "  training['text']  = training_x\n",
        "\n",
        "  val['label'] = val_y\n",
        "  val['text']  = val_x\n",
        "\n",
        "  test['label'] = test_y\n",
        "  test['text']  = test_x\n",
        "\n",
        "  # val = pd.DataFrame([training_x, training_y]).T\n",
        "  # test = pd.DataFrame([training_x, training_y]).T\n",
        "\n",
        "  # training.columns = ['label', 'text']\n",
        "  # val.columns = ['label', 'text']\n",
        "  # test.columns = ['label', 'text']\n",
        "  print(training)\n",
        "  print(val)\n",
        "  print(test)\n",
        "  \n",
        "  assert training.shape[0] + val.shape[0] + test.shape[0] == length\n",
        "#   print(len(training) + len(val) + len(test))\n",
        "\n",
        "#   training.to_csv(\"drive/My Drive/TextMining_Project/pl_data/{}/train.csv\".format(outputfolder), index=False, header=False)\n",
        "#   val.to_csv(\"drive/My Drive/TextMining_Project/pl_data/{}/val.csv\".format(outputfolder), index=False, header=False)\n",
        "#   test.to_csv(\"drive/My Drive/TextMining_Project/pl_data/{}/test.csv\".format(outputfolder), index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3O9T9vkbk6tE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "7S8sP7lXlQqd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stopwords = list(pd.read_table('/content/drive/My Drive/TextMining_Project/stopwords-nl.txt', header=None).iloc[:,0])\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "trainedclfs = []\n",
        "\n",
        "dataset_means = []\n",
        "\n",
        "for dataset in ['news', 'tw', 'yt']:\n",
        "  \n",
        "  train = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/train.csv'.format(dataset), header=None)\n",
        "  val = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/val.csv'.format(dataset), header=None)\n",
        "  test = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/test.csv'.format(dataset), header=None)\n",
        "\n",
        "  \n",
        "  \n",
        "  train.columns = ['class', 'text']\n",
        "  val.columns = ['class', 'text']\n",
        "  test.columns = ['class', 'text']\n",
        "\n",
        "  trainval = pd.concat([train, val])\n",
        "  \n",
        "  kf = KFold(n_splits=5)\n",
        "  \n",
        "  accuracy_folds = []\n",
        "  \n",
        "  for e, (train_index, val_index) in enumerate(kf.split(trainval)): \n",
        "    print(\"Fold number: \", e+1)\n",
        "    accuracy_fold = []\n",
        "    train_fold, val_fold = trainval.iloc[train_index, :], trainval.iloc[val_index, :]\n",
        "  \n",
        "    vectorizer = CountVectorizer(min_df=100, analyzer='char', lowercase=False, ngram_range=(1, 3))\n",
        "    train_data = vectorizer.fit_transform(train_fold['text'])\n",
        "    val_data = vectorizer.transform(val_fold['text'])\n",
        "    test_data = vectorizer.transform(test['text'])\n",
        "    \n",
        "    print(train_data.shape)\n",
        "\n",
        "    wordvectorizer = TfidfVectorizer(min_df=100, analyzer='word', lowercase=False, ngram_range=(1,3))\n",
        "    train_data_word = wordvectorizer.fit_transform(train_fold['text'])\n",
        "    val_data_word = wordvectorizer.transform(val_fold['text'])\n",
        "    test_data_word = wordvectorizer.transform(test['text'])\n",
        "  \n",
        "    print(train_data_word.shape)\n",
        "    train_data = np.hstack([train_data.toarray(), train_data_word.toarray()])\n",
        "    val_data = np.hstack([val_data.toarray(), val_data_word.toarray()])\n",
        "    test_data = np.hstack([test_data.toarray(), test_data_word.toarray()])\n",
        "\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_data)\n",
        "\n",
        "    train_scaled = scaler.transform(train_data)\n",
        "    val_scaled = scaler.transform(val_data)\n",
        "    test_scaled = scaler.transform(test_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    clf = LogisticRegression(C=1)\n",
        "    clf.fit(train_scaled, train_fold['class'])\n",
        "    trainedclfs.append(clf)\n",
        "\n",
        "    trainpreds = clf.predict(train_scaled)\n",
        "    valpreds = clf.predict(val_scaled)\n",
        "    testpreds = clf.predict(test_scaled)\n",
        "    train_acc = accuracy_score(train_fold['class'], trainpreds)\n",
        "    val_acc   = accuracy_score(val_fold['class'], valpreds)\n",
        "    test_acc  = accuracy_score(test['class'], testpreds)\n",
        "\n",
        "    print(\"Train acc: \", train_acc, \"Val acc: \", val_acc, \"Test acc: \", test_acc)\n",
        "    \n",
        "    accuracy_fold.append(train_acc)\n",
        "    accuracy_fold.append(val_acc)\n",
        "    accuracy_fold.append(test_acc)\n",
        "    \n",
        "    \n",
        "    for dataset_transfer in ['news', 'tw', 'yt']:\n",
        "      if dataset is not dataset_transfer:\n",
        "        test_transfer = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/test.csv'.format(dataset_transfer), header=None)\n",
        "        test_transfer.columns = ['class', 'text']\n",
        "\n",
        "        test_transfer_data = vectorizer.transform(test_transfer['text'])\n",
        "        test_transfer_data_word = wordvectorizer.transform(test_transfer['text'])\n",
        "        test_transfer_data = np.hstack([test_transfer_data.toarray(), test_transfer_data_word.toarray()])\n",
        "\n",
        "        test_transfer_scaled = scaler.transform(test_transfer_data)\n",
        "        transferpreds = clf.predict(test_transfer_data)\n",
        "        transfer_acc = accuracy_score(test_transfer['class'], transferpreds)\n",
        "        print(dataset, \"->\", dataset_transfer, transfer_acc)\n",
        "        \n",
        "        accuracy_fold.append(transfer_acc)\n",
        "    accuracy_folds.append(accuracy_fold)\n",
        "  dataset_means.append(accuracy_folds)\n",
        "\n",
        "  \n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/TextMining_Project/logreg_kfold_results.pickle', 'wb') as handle:\n",
        "    pickle.dump(dataset_means, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SnHPao59lbM1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression with LSA inputs"
      ]
    },
    {
      "metadata": {
        "id": "av-AIkc7leor",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stopwords = list(pd.read_table('/content/drive/My Drive/TextMining_Project/stopwords-nl.txt', header=None).iloc[:,0])\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "trainedclfs = []\n",
        "\n",
        "dataset_means = []\n",
        "\n",
        "for dataset in ['news', 'tw', 'yt']:\n",
        "  \n",
        "  train = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/train.csv'.format(dataset), header=None)\n",
        "  val = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/val.csv'.format(dataset), header=None)\n",
        "  test = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/test.csv'.format(dataset), header=None)\n",
        "\n",
        "  \n",
        "  \n",
        "  train.columns = ['class', 'text']\n",
        "  val.columns = ['class', 'text']\n",
        "  test.columns = ['class', 'text']\n",
        "\n",
        "  trainval = pd.concat([train, val])\n",
        "  \n",
        "  kf = KFold(n_splits=5)\n",
        "  \n",
        "  accuracy_folds = []\n",
        "  \n",
        "  for e, (train_index, val_index) in enumerate(kf.split(trainval)): \n",
        "    print(\"Fold number: \", e+1)\n",
        "    accuracy_fold = []\n",
        "    train_fold, val_fold = trainval.iloc[train_index, :], trainval.iloc[val_index, :]      \n",
        "    \n",
        "\n",
        "    \n",
        "    wordvectorizer = CountVectorizer(min_df=2, analyzer='word', lowercase=True, ngram_range=(1,3), stop_words=stopwords)\n",
        "    train_data_word = wordvectorizer.fit_transform(train_fold['text'])\n",
        "    val_data_word = wordvectorizer.transform(val_fold['text'])\n",
        "    test_data_word = wordvectorizer.transform(test['text'])\n",
        "    \n",
        "    print(train_data_word.shape)\n",
        "    # SVD represent documents and terms in vectors \n",
        "    svd_model_topics = TruncatedSVD(n_components=30, algorithm='randomized', n_iter=100, random_state=4711)\n",
        "\n",
        "    svd_model_topics.fit(train_data_word)\n",
        "    train_topics = svd_model_topics.transform(train_data_word)\n",
        "    val_topics = svd_model_topics.transform(val_data_word)\n",
        "    test_topics = svd_model_topics.transform(test_data_word)\n",
        "\n",
        "\n",
        "    train_data = np.hstack([train_topics])\n",
        "    val_data = np.hstack([val_topics])\n",
        "    test_data = np.hstack([test_topics])\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_data)\n",
        "\n",
        "    train_scaled = scaler.transform(train_data)\n",
        "    val_scaled = scaler.transform(val_data)\n",
        "    test_scaled = scaler.transform(test_data)\n",
        "    print(train_data.shape)\n",
        "\n",
        "    clf = LogisticRegression(C=1)\n",
        "    clf.fit(train_scaled, train_fold['class'])\n",
        "    trainedclfs.append(clf)\n",
        "\n",
        "    trainpreds = clf.predict(train_scaled)\n",
        "    valpreds = clf.predict(val_scaled)\n",
        "    testpreds = clf.predict(test_scaled)\n",
        "    train_acc = accuracy_score(train_fold['class'], trainpreds)\n",
        "    val_acc   = accuracy_score(val_fold['class'], valpreds)\n",
        "    test_acc  = accuracy_score(test['class'], testpreds)\n",
        "\n",
        "    print(\"Train acc: \", train_acc, \"Val acc: \", val_acc, \"Test acc: \", test_acc)\n",
        "    \n",
        "    accuracy_fold.append(train_acc)\n",
        "    accuracy_fold.append(val_acc)\n",
        "    accuracy_fold.append(test_acc)\n",
        "    \n",
        "    \n",
        "    for dataset_transfer in ['news', 'tw', 'yt']:\n",
        "      if dataset is not dataset_transfer:\n",
        "        test_transfer = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/test.csv'.format(dataset_transfer), header=None)\n",
        "        test_transfer.columns = ['class', 'text']\n",
        "\n",
        "#         test_transfer_data = vectorizer.transform(test_transfer['text'])\n",
        "        test_transfer_data_word = wordvectorizer.transform(test_transfer['text'])\n",
        "#         test_transfer_lsa = svd_model_lsa.transform(test_transfer_data)\n",
        "        test_transfer_topics = svd_model_topics.transform(test_transfer_data_word)\n",
        "        test_transfer_data = np.hstack([test_transfer_topics])\n",
        "\n",
        "        test_transfer_scaled = scaler.transform(test_transfer_data)\n",
        "        transferpreds = clf.predict(test_transfer_data)\n",
        "        transfer_acc = accuracy_score(test_transfer['class'], transferpreds)\n",
        "        print(dataset, \"->\", dataset_transfer, transfer_acc)\n",
        "        \n",
        "        accuracy_fold.append(transfer_acc)\n",
        "    accuracy_folds.append(accuracy_fold)\n",
        "  dataset_means.append(accuracy_folds)\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/TextMining_Project/logreg_lsa_kfold_results.pickle', 'wb') as handle:\n",
        "    pickle.dump(dataset_means, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v13qvF8Yjtw8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# BidirectionalLSTM"
      ]
    },
    {
      "metadata": {
        "id": "gvXZ_TZgN-du",
        "colab_type": "code",
        "outputId": "9d11d759-7fd8-44b7-9a7e-0392c172d5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LoIplrgROJdY",
        "colab_type": "code",
        "outputId": "ac247daf-9af7-497d-abd0-7af392ce12e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OHAT8eXBY18J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## K-folding"
      ]
    },
    {
      "metadata": {
        "id": "1i9I7dNgO6lE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "dataset_means = []\n",
        "for dataset in ['news', 'tw', 'yt']:\n",
        "  train = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/train.csv'.format(dataset), header=None)\n",
        "  val = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/val.csv'.format(dataset), header=None)\n",
        "  test = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/test.csv'.format(dataset), header=None)\n",
        "  train.columns = ['class', 'text']\n",
        "  val.columns = ['class', 'text']\n",
        "  test.columns = ['class', 'text']\n",
        "\n",
        "  \n",
        "  trainval = pd.concat([train, val])\n",
        "  \n",
        "  kf = KFold(n_splits=5)\n",
        "  \n",
        "  accuracy_folds = []\n",
        "  \n",
        "  for e, (train_index, val_index) in enumerate(kf.split(trainval)): \n",
        "    print(\"Fold number: \", e+1)\n",
        "    accuracy_fold = []\n",
        "    train_fold, val_fold = trainval.iloc[train_index, :], trainval.iloc[val_index, :]\n",
        "    \n",
        "    def RNN():\n",
        "        inputs = Input(name='inputs',shape=[max_len])\n",
        "        layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "        layer = Bidirectional(LSTM(16))(layer)\n",
        "        layer = Dense(10,name='FC1')(layer)\n",
        "        layer = Activation('relu')(layer)\n",
        "        layer = Dropout(0.3)(layer)\n",
        "        layer = Dense(1,name='out_layer')(layer)\n",
        "        layer = Activation('sigmoid')(layer)\n",
        "        model = Model(inputs=inputs,outputs=layer)\n",
        "        return model\n",
        "    if dataset==\"news\":\n",
        "      max_len = 200\n",
        "    else:\n",
        "      max_len = 20\n",
        "\n",
        "    max_words = 3000\n",
        "\n",
        "    tok = Tokenizer(num_words=max_words)\n",
        "    tok.fit_on_texts(train['text'])\n",
        "    train_sequences = tok.texts_to_sequences(train_fold['text'])\n",
        "    val_sequences = tok.texts_to_sequences(val_fold['text'])\n",
        "    test_sequences = tok.texts_to_sequences(test['text'])\n",
        "\n",
        "    sequences_matrix = sequence.pad_sequences(train_sequences,maxlen=max_len)\n",
        "    val_sequences_matrix = sequence.pad_sequences(val_sequences,maxlen=max_len)\n",
        "    test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
        "\n",
        "    model = RNN()\n",
        "    model.summary()\n",
        "    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
        "\n",
        "    model.fit(sequences_matrix,train_fold['class'],batch_size=128,epochs=10,\n",
        "              validation_data=(val_sequences_matrix, val_fold['class']),callbacks=[EarlyStopping(patience=2, monitor='val_loss',min_delta=0.0001)])\n",
        "\n",
        "    train_acc = model.evaluate(sequences_matrix, train_fold['class'])[1]\n",
        "    val_acc = model.evaluate(val_sequences_matrix, val_fold['class'])[1]\n",
        "    test_acc = model.evaluate(test_sequences_matrix, test['class'])[1]\n",
        "    accuracy_fold.append(train_acc)\n",
        "    accuracy_fold.append(val_acc)\n",
        "    accuracy_fold.append(test_acc)\n",
        "    print(train_acc, val_acc, test_acc)\n",
        "\n",
        "\n",
        "    for dataset_transfer in ['news', 'tw', 'yt']:\n",
        "        if dataset is not dataset_transfer:\n",
        "          test_transfer = pd.read_csv('/content/drive/My Drive/TextMining_Project/pl_data/{}/test.csv'.format(dataset_transfer), header=None)\n",
        "          test_transfer.columns = ['class', 'text']\n",
        "\n",
        "          test_transfer_sequences = tok.texts_to_sequences(test_transfer['text'])\n",
        "          test_transfer_sequences_matrix = sequence.pad_sequences(test_transfer_sequences,maxlen=max_len)\n",
        "        \n",
        "          transfer_acc = model.evaluate(test_transfer_sequences_matrix, test_transfer['class'])[1]\n",
        "\n",
        "          print(dataset, \"->\", dataset_transfer, transfer_acc)\n",
        "          accuracy_fold.append(transfer_acc)\n",
        "          \n",
        "        \n",
        "    accuracy_folds.append(accuracy_fold)\n",
        "  dataset_means.append(accuracy_folds)\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/My Drive/TextMining_Project/lstm_kfold_results.pickle', 'wb') as handle:\n",
        "    pickle.dump(dataset_means, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}